{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqR1j8BSFPSB",
    "outputId": "094e025b-d322-45c9-b377-60a2b1077a5e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in ./anaconda3/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in ./anaconda3/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in ./anaconda3/lib/python3.11/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in ./anaconda3/lib/python3.11/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./anaconda3/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_df_eUYc-bJX",
    "outputId": "09e0c8b1-72e9-4d25-8026-5241b19b06a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /dgxa_home/se20uari151/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Brown Corpus data before preprocessing:\n",
      "                                      tokenized_text\n",
      "0  Furthermore , as an encouragement to revisioni...\n",
      "1  The Unitarian clergy were an exclusive club of...\n",
      "2  Ezra Stiles Gannett , an honorable representat...\n",
      "3  Even so , Gannett judiciously argued , the Ass...\n",
      "4  We today are not entitled to excoriate honest ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load SimLex999 data\n",
    "simlex_data = pd.read_csv(\"/dgxa_home/se20uari151//SimLex-999.txt\", delimiter='\\t')\n",
    "\n",
    "# Load Brown Corpus data\n",
    "brown_data = pd.read_csv('/dgxa_home/se20uari151/brown.csv')\n",
    "\n",
    "print(\"Sample of Brown Corpus data before preprocessing:\")\n",
    "print(brown_data[['tokenized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6xSygDCBl37",
    "outputId": "072f248c-4c87-4414-f4e8-0cbcfc008923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Preprocessing:\n",
      "0    [furthermore, encouragement, revisionist, thin...\n",
      "1    [unitarian, clergy, exclusive, club, cultivate...\n",
      "2    [ezra, stiles, gannett, honorable, representat...\n",
      "3    [even, gannett, judiciously, argued, associati...\n",
      "4    [today, entitled, excoriate, honest, men, beli...\n",
      "Name: preprocessed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    tokens = [word.lower() for word in text.split() if word.isalpha()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply text preprocessing to each sentence in the Brown Corpus\n",
    "brown_data['preprocessed_text'] = brown_data['tokenized_text'].apply(preprocess_text)\n",
    "\n",
    "# Display the dataset after preprocessing\n",
    "print(\"\\nAfter Preprocessing:\")\n",
    "print(brown_data['preprocessed_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKsjBvbABl0h",
    "outputId": "d9704a6a-d8ec-4350-d69b-03f2e572f084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 word  vectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "glove_dir='/dgxa_home/se20uari151'\n",
    "\n",
    "embeddings_index={}\n",
    "f=open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "  values=line.split()\n",
    "  word=values[0]\n",
    "  coefs=np.asarray(values[1:], dtype='float32')\n",
    "  embeddings_index[word]=coefs\n",
    "f.close()\n",
    "\n",
    "print('%s word  vectors'%len(embeddings_index))\n",
    "\n",
    "# embeddings_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnsFZK8zBlw8",
    "outputId": "b2410aeb-5a3f-4b87-edb0-cc5c2df7c25a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text  \\\n",
      "0  [furthermore, encouragement, revisionist, thin...   \n",
      "1  [unitarian, clergy, exclusive, club, cultivate...   \n",
      "2  [ezra, stiles, gannett, honorable, representat...   \n",
      "3  [even, gannett, judiciously, argued, associati...   \n",
      "4  [today, entitled, excoriate, honest, men, beli...   \n",
      "\n",
      "                                     word_embeddings  \n",
      "0  [[-0.158, 0.15186, -0.098325, 0.31868, -0.0931...  \n",
      "1  [[0.14219, 0.68774, -0.84726, -0.027145, -0.41...  \n",
      "2  [[0.30293, 0.33103, -1.1076, -0.1375, -0.35773...  \n",
      "3  [[-0.15308, 0.63194, 0.65512, -0.30706, -0.239...  \n",
      "4  [[-0.19939, 0.37846, 0.52093, 0.28347, -0.1898...  \n"
     ]
    }
   ],
   "source": [
    "# Create word embeddings for Brown Corpus using GloVe\n",
    "embedding_dim = 100  # Assuming you are using GloVe with 100-dimensional vectors\n",
    "\n",
    "def get_embedding(word):\n",
    "    return embeddings_index.get(word, np.zeros(embedding_dim))\n",
    "\n",
    "brown_data['word_embeddings'] = brown_data['preprocessed_text'].apply(lambda words: [get_embedding(word) for word in words])\n",
    "\n",
    "# Display a few entries to verify the embeddings\n",
    "print(brown_data[['preprocessed_text', 'word_embeddings']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpXuDsl7Bs2z",
    "outputId": "9193975e-aa58-4a5c-c419-a6d105581f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "(14, 100)\n",
      "16\n",
      "(16, 100)\n",
      "9\n",
      "(9, 100)\n",
      "Total number of rows in 'word_embeddings': 57340\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(len(brown_data['preprocessed_text'][0]))\n",
    "\n",
    "print(np.vstack(brown_data['word_embeddings'][0]).shape)\n",
    "\n",
    "\n",
    "print(len(brown_data['preprocessed_text'][1]))\n",
    "print(np.vstack(brown_data['word_embeddings'][1]).shape)\n",
    "\n",
    "\n",
    "print(len(brown_data['preprocessed_text'][55160]))\n",
    "print(np.vstack(brown_data['word_embeddings'][55160]).shape)\n",
    "\n",
    "total_rows = len(brown_data['word_embeddings'])\n",
    "print(\"Total number of rows in 'word_embeddings':\", total_rows)\n",
    "\n",
    "print(type(np.vstack(brown_data['word_embeddings'][0]).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "gvXpjwvRBsy6",
    "outputId": "620e9d97-ed84-4e81-ee42-dcc895b3edee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in 'rrrrrpreprocessed_text': False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>word_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furthermore</td>\n",
       "      <td>[-0.158, 0.15186, -0.098325, 0.31868, -0.09317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>encouragement</td>\n",
       "      <td>[0.72701, 0.58038, -0.12419, -0.027612, 0.4040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>revisionist</td>\n",
       "      <td>[0.12806, 0.86505, 0.16681, 0.21417, -0.25038,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thinking</td>\n",
       "      <td>[0.56354, 0.28124, 0.62225, 0.2077, -0.47417, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>manifestly</td>\n",
       "      <td>[0.38598, -0.14472, -0.15539, 0.12179, -0.1323...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preprocessed_text                                    word_embeddings\n",
       "0       furthermore  [-0.158, 0.15186, -0.098325, 0.31868, -0.09317...\n",
       "1     encouragement  [0.72701, 0.58038, -0.12419, -0.027612, 0.4040...\n",
       "2       revisionist  [0.12806, 0.86505, 0.16681, 0.21417, -0.25038,...\n",
       "3          thinking  [0.56354, 0.28124, 0.62225, 0.2077, -0.47417, ...\n",
       "4        manifestly  [0.38598, -0.14472, -0.15539, 0.12179, -0.1323..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrearranged_data=pd.read_csv('/dgxa_home/se20uari151/token.csv')\n",
    "\n",
    "# Drop duplicates based on 'preprocessed_text'\n",
    "rrearranged_data = rrearranged_data.drop_duplicates(subset=['preprocessed_text'])\n",
    "\n",
    "duplicates = rrearranged_data['preprocessed_text'].duplicated()\n",
    "print(\"Duplicates in 'rrrrrpreprocessed_text':\", duplicates.any())\n",
    "\n",
    "# Assuming your new dataset is named 'new_dataset' and has a column 'preprocessed_text'\n",
    "rrearranged_data['word_embeddings'] = rrearranged_data['preprocessed_text'].apply(lambda text: [get_embedding(word) for word in text.split()] if isinstance(text, str) else [])\n",
    "\n",
    "\n",
    "# Assuming 'word_embeddings' column is already populated in your DataFrame\n",
    "rrearranged_data['word_embeddings'] = rrearranged_data['word_embeddings'].apply(lambda embeddings_list: np.concatenate(embeddings_list) if embeddings_list else [])\n",
    "\n",
    "# Assuming 'rearranged_data' is your DataFrame\n",
    "rrearranged_data = rrearranged_data.drop(columns=['Unnamed: 1'])\n",
    "\n",
    "rrearranged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOSEo7_lBsuY",
    "outputId": "2ff39370-46f4-494d-b0f3-9d6fba35a9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "identified\n",
      "116407    NaN\n",
      "Name: preprocessed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(np.vstack(rrearranged_data['word_embeddings'][19225]).shape)\n",
    "\n",
    "print((rrearranged_data['preprocessed_text'][19225]))\n",
    "print((rrearranged_data['preprocessed_text'][19225:19226]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8_inv3WBls-",
    "outputId": "20d90fc4-efd8-407d-b3af-1d769c88c1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values:\n",
      "       preprocessed_text word_embeddings\n",
      "116407               NaN              []\n",
      "DataFrame after dropping NaN values:\n",
      "  preprocessed_text                                    word_embeddings\n",
      "0       furthermore  [-0.158, 0.15186, -0.098325, 0.31868, -0.09317...\n",
      "1     encouragement  [0.72701, 0.58038, -0.12419, -0.027612, 0.4040...\n",
      "2       revisionist  [0.12806, 0.86505, 0.16681, 0.21417, -0.25038,...\n",
      "3          thinking  [0.56354, 0.28124, 0.62225, 0.2077, -0.47417, ...\n",
      "4        manifestly  [0.38598, -0.14472, -0.15539, 0.12179, -0.1323...\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the DataFrame\n",
    "nan_rows = rrearranged_data[rrearranged_data.isnull().any(axis=1)]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "rrearranged_data = rrearranged_data.dropna()\n",
    "\n",
    "# Display the rows with NaN values (if any)\n",
    "print(\"Rows with NaN values:\")\n",
    "print(nan_rows)\n",
    "\n",
    "# Display the DataFrame after dropping NaN values\n",
    "print(\"DataFrame after dropping NaN values:\")\n",
    "print(rrearranged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbB4kBCpB06_",
    "outputId": "e4b1bbb9-8e3c-4cc7-ec19-505f6f1da9d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  preprocessed_text                                    word_embeddings\n",
      "0       furthermore  [-0.158, 0.15186, -0.098325, 0.31868, -0.09317...\n",
      "1     encouragement  [0.72701, 0.58038, -0.12419, -0.027612, 0.4040...\n",
      "2       revisionist  [0.12806, 0.86505, 0.16681, 0.21417, -0.25038,...\n",
      "3          thinking  [0.56354, 0.28124, 0.62225, 0.2077, -0.47417, ...\n",
      "4        manifestly  [0.38598, -0.14472, -0.15539, 0.12179, -0.1323...\n",
      "0          furthermore\n",
      "1        encouragement\n",
      "2          revisionist\n",
      "3             thinking\n",
      "4           manifestly\n",
      "             ...      \n",
      "40094    pharmacopoeia\n",
      "40095     redefinition\n",
      "40096              usp\n",
      "40097          mussett\n",
      "40098       equipotent\n",
      "Name: preprocessed_text, Length: 40099, dtype: object\n"
     ]
    }
   ],
   "source": [
    "rrearranged_data = rrearranged_data.reset_index(drop=True)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(rrearranged_data.head())\n",
    "print((rrearranged_data.loc[:, 'preprocessed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcJQkZ4wB02p",
    "outputId": "f58d9491-4fb8-4c54-cfc6-f31af03d0f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equipotent\n"
     ]
    }
   ],
   "source": [
    "print((rrearranged_data['preprocessed_text'][40098]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "F7dBsmaQEvCc",
    "outputId": "4b3e04af-5fca-4295-d1ad-bb125ab31111"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>word_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furthermore</td>\n",
       "      <td>[-0.158, 0.15186, -0.098325, 0.31868, -0.09317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>encouragement</td>\n",
       "      <td>[0.72701, 0.58038, -0.12419, -0.027612, 0.4040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>revisionist</td>\n",
       "      <td>[0.12806, 0.86505, 0.16681, 0.21417, -0.25038,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thinking</td>\n",
       "      <td>[0.56354, 0.28124, 0.62225, 0.2077, -0.47417, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>manifestly</td>\n",
       "      <td>[0.38598, -0.14472, -0.15539, 0.12179, -0.1323...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preprocessed_text                                    word_embeddings\n",
       "0       furthermore  [-0.158, 0.15186, -0.098325, 0.31868, -0.09317...\n",
       "1     encouragement  [0.72701, 0.58038, -0.12419, -0.027612, 0.4040...\n",
       "2       revisionist  [0.12806, 0.86505, 0.16681, 0.21417, -0.25038,...\n",
       "3          thinking  [0.56354, 0.28124, 0.62225, 0.2077, -0.47417, ...\n",
       "4        manifestly  [0.38598, -0.14472, -0.15539, 0.12179, -0.1323..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrearranged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          furthermore\n",
      "1        encouragement\n",
      "2          revisionist\n",
      "3             thinking\n",
      "4           manifestly\n",
      "             ...      \n",
      "40093       thyrotoxic\n",
      "40094    pharmacopoeia\n",
      "40095     redefinition\n",
      "40096              usp\n",
      "40097          mussett\n",
      "Name: preprocessed_text, Length: 40098, dtype: object\n",
      "          0         1         2         3         4         5         6      \\\n",
      "0      1.000000  0.247114 -0.015581  0.417657  0.161974  0.386030  0.373484   \n",
      "1      0.247114  1.000000  0.131388  0.348723  0.068426  0.230121  0.272027   \n",
      "2     -0.015581  0.131388  1.000000  0.225604  0.139131  0.079546  0.134561   \n",
      "3      0.417657  0.348723  0.225604  1.000000  0.036609  0.373480  0.540338   \n",
      "4      0.161974  0.068426  0.139131  0.036609  1.000000  0.184054  0.143845   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "40093  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "40094  0.020284 -0.052673  0.214876 -0.203851  0.126293  0.044559 -0.141453   \n",
      "40095 -0.054721  0.036576  0.090212  0.023937  0.206170  0.042126 -0.020062   \n",
      "40096 -0.032683 -0.086783 -0.002673 -0.218538  0.086483 -0.092828 -0.022658   \n",
      "40097  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "          7         8         9      ...  40088     40089  40090     40091  \\\n",
      "0      0.164207  0.345138  0.407546  ...    0.0 -0.165367    0.0 -0.069174   \n",
      "1      0.201575  0.117106  0.264732  ...    0.0 -0.009274    0.0  0.071159   \n",
      "2      0.056937  0.058321  0.012477  ...    0.0  0.125772    0.0  0.133213   \n",
      "3      0.166712  0.312588  0.610313  ...    0.0 -0.046116    0.0 -0.118255   \n",
      "4     -0.009928  0.124887  0.094337  ...    0.0 -0.129353    0.0  0.117200   \n",
      "...         ...       ...       ...  ...    ...       ...    ...       ...   \n",
      "40093  0.000000  0.000000  0.000000  ...    0.0  0.000000    0.0  0.000000   \n",
      "40094 -0.028550  0.024907 -0.367134  ...    0.0  0.062987    0.0  0.232561   \n",
      "40095  0.165957  0.156377 -0.008445  ...    0.0 -0.038056    0.0  0.054631   \n",
      "40096  0.172397 -0.014028 -0.250366  ...    0.0  0.059209    0.0  0.065519   \n",
      "40097  0.000000  0.000000  0.000000  ...    0.0  0.000000    0.0  0.000000   \n",
      "\n",
      "          40092  40093     40094     40095     40096  40097  \n",
      "0      0.178381    0.0  0.020284 -0.054721 -0.032683    0.0  \n",
      "1     -0.013862    0.0 -0.052673  0.036576 -0.086783    0.0  \n",
      "2     -0.139433    0.0  0.214876  0.090212 -0.002673    0.0  \n",
      "3      0.013960    0.0 -0.203851  0.023937 -0.218538    0.0  \n",
      "4      0.321924    0.0  0.126293  0.206170  0.086483    0.0  \n",
      "...         ...    ...       ...       ...       ...    ...  \n",
      "40093  0.000000    0.0  0.000000  0.000000  0.000000    0.0  \n",
      "40094 -0.046316    0.0  1.000000 -0.005340  0.371171    0.0  \n",
      "40095 -0.026447    0.0 -0.005340  1.000000  0.003734    0.0  \n",
      "40096 -0.030454    0.0  0.371171  0.003734  1.000000    0.0  \n",
      "40097  0.000000    0.0  0.000000  0.000000  0.000000    0.0  \n",
      "\n",
      "[40098 rows x 40098 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "upto=40098\n",
    "# Take the first 14 rows of word embeddings\n",
    "subset_embeddings = rrearranged_data['word_embeddings'][:upto]\n",
    "\n",
    "# Convert the subset of embeddings to a NumPy array\n",
    "subset_embeddings_array = np.vstack(subset_embeddings)\n",
    "\n",
    "# Calculate cosine similarity matrix for the subset\n",
    "cosine_sim_matrix = cosine_similarity(subset_embeddings_array)\n",
    "\n",
    "# Subset the cosine similarity matrix for the first 14 rows\n",
    "cosine_sim_matrix_subset = cosine_sim_matrix[:upto, :upto]\n",
    "\n",
    "# Optionally, you can convert the similarity matrix to a DataFrame for better visualization\n",
    "cosine_sim_df_subset = pd.DataFrame(cosine_sim_matrix_subset, index=range(upto), columns=range(upto))\n",
    "\n",
    "# Display the cosine similarity DataFrame for the subset\n",
    "print(rrearranged_data['preprocessed_text'][:upto])\n",
    "print(cosine_sim_df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'flower' and 'endurance': -0.01657593563095525\n"
     ]
    }
   ],
   "source": [
    "word1 = \"flower\"\n",
    "word2 = \"endurance\"\n",
    "\n",
    "# Check if the words exist in the preprocessed_text column\n",
    "if word1 in rrearranged_data['preprocessed_text'].values and word2 in rrearranged_data['preprocessed_text'].values:\n",
    "    # Get the index numbers in the preprocessed_text column\n",
    "    index1 = rrearranged_data.index[rrearranged_data['preprocessed_text'] == word1].tolist()[0]\n",
    "    index2 = rrearranged_data.index[rrearranged_data['preprocessed_text'] == word2].tolist()[0]\n",
    "\n",
    "    # Get the similarity value from the cosine_sim_df_subset DataFrame\n",
    "    similarity_value = cosine_sim_df_subset.iloc[index1, index2]\n",
    "\n",
    "    # Map the similarity value to the range [0, 10]\n",
    "    mapped_similarity = similarity_value\n",
    "\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {mapped_similarity}\")\n",
    "else:\n",
    "    print(f\"One or both of the words '{word1}' and '{word2}' not found in the preprocessed_text column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcWTLrZAGYmC",
    "outputId": "1a8e17ff-58fc-4a82-bb81-4fb553407172",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One or both of the words 'disorganize' and 'organize' not found in the preprocessed_text column.\n",
      "One or both of the words 'do' and 'happen' not found in the preprocessed_text column.\n",
      "One or both of the words 'do' and 'quit' not found in the preprocessed_text column.\n",
      "      word1        word2  SimLex999  mapped_similarity\n",
      "0       old          new       1.58           8.216244\n",
      "1     smart  intelligent       9.20           8.776366\n",
      "2      hard    difficult       8.77           8.926273\n",
      "3     happy     cheerful       9.55           7.729968\n",
      "4      hard         easy       0.95           8.833839\n",
      "..      ...          ...        ...                ...\n",
      "994    join      acquire       2.85           7.323556\n",
      "995    send       attend       1.67           7.504681\n",
      "996  gather       attend       4.80           7.701219\n",
      "997  absorb     withdraw       2.97           6.445788\n",
      "998  attend       arrive       6.08           7.969202\n",
      "\n",
      "[999 rows x 4 columns]\n",
      "Correlation between SimLex999 and mapped_similarity: 0.3244313743300278\n"
     ]
    }
   ],
   "source": [
    "# Linear transformation function\n",
    "def transform_similarity(similarity):\n",
    "    # Map from [-1, 1] to [0, 10]\n",
    "    return (similarity + 1) * 5\n",
    "\n",
    "def get_similarity_between_words(word1, word2, rearranged_data, cosine_sim_df_subset):\n",
    "    if word1 in rearranged_data['preprocessed_text'].values and word2 in rearranged_data['preprocessed_text'].values:\n",
    "        # Get the index numbers in the preprocessed_text column\n",
    "        index1 = rearranged_data.index[rearranged_data['preprocessed_text'] == word1].tolist()[0]\n",
    "        index2 = rearranged_data.index[rearranged_data['preprocessed_text'] == word2].tolist()[0]\n",
    "\n",
    "        # Get the similarity value from the cosine_sim_df_subset DataFrame\n",
    "        similarity_value = cosine_sim_df_subset.iloc[index1, index2]\n",
    "\n",
    "        # Map the similarity value to the range [0, 10]\n",
    "        mapped_similarity = transform_similarity(similarity_value)\n",
    "\n",
    "        return mapped_similarity\n",
    "    else:\n",
    "        print(f\"One or both of the words '{word1}' and '{word2}' not found in the preprocessed_text column.\")\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'word1' and 'word2' columns of SimLex999\n",
    "simlex_data['mapped_similarity'] = simlex_data.apply(lambda row: get_similarity_between_words(row['word1'], row['word2'], rrearranged_data, cosine_sim_df_subset), axis=1)\n",
    "\n",
    "# Display the updated SimLex999 dataset with mapped similarity values\n",
    "print(simlex_data[['word1', 'word2','SimLex999', 'mapped_similarity']])\n",
    "\n",
    "# Extract the relevant columns for correlation\n",
    "simlex_subset_for_correlation = simlex_data[['SimLex999', 'mapped_similarity']]\n",
    "\n",
    "# Drop rows with NaN values (if any)\n",
    "simlex_subset_for_correlation = simlex_subset_for_correlation.dropna()\n",
    "\n",
    "# Calculate the correlation\n",
    "correlation = np.corrcoef(simlex_subset_for_correlation['SimLex999'], simlex_subset_for_correlation['mapped_similarity'])[0, 1]\n",
    "\n",
    "print(f'Correlation between SimLex999 and mapped_similarity: {correlation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word1      word2  SimLex999  mapped_similarity\n",
      "723       flower  endurance       0.40           4.917120\n",
      "744         hymn      straw       0.40           4.920208\n",
      "82        modest    ashamed       2.65           4.993611\n",
      "720    endurance       band       0.40           5.231201\n",
      "521          pot  appliance       2.53           5.231400\n",
      "..           ...        ...        ...                ...\n",
      "777           go       come       2.42           9.639308\n",
      "359      brother        son       3.48           9.688039\n",
      "855  disorganize   organize       1.45                NaN\n",
      "865           do     happen       4.23                NaN\n",
      "924           do       quit       1.17                NaN\n",
      "\n",
      "[999 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(simlex_data[['word1', 'word2', 'SimLex999', 'mapped_similarity']].sort_values(by='mapped_similarity'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Assuming n_clusters is the number of clusters you want\n",
    "# n_clusters = 5\n",
    "# kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# # Fit the KMeans model on the cosine similarity matrix\n",
    "# clusters = kmeans.fit_predict(cosine_sim_df_subset)\n",
    "\n",
    "# # Add the cluster labels to the DataFrame\n",
    "# rrearranged_data['cluster'] = clusters\n",
    "\n",
    "# # Display the clusters and their representative words\n",
    "# for cluster_label in range(n_clusters):\n",
    "#     cluster_words = rrearranged_data.loc[rrearranged_data['cluster'] == cluster_label, 'preprocessed_text']\n",
    "#     print(f\"\\nCluster {cluster_label + 1}: {', '.join(cluster_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have ground truth similarity scores in simlex_data['SimLex999']\n",
    "# ground_truth_scores = simlex_data['SimLex999'].values\n",
    "\n",
    "# # Flatten the upper triangle of the cosine similarity matrix (excluding the diagonal)\n",
    "# flat_cosine_sim = cosine_sim_matrix[np.triu_indices(40098, k=1)]\n",
    "\n",
    "# # Evaluate the correlation between ground truth scores and cosine similarity\n",
    "# correlation = np.corrcoef(flat_cosine_sim, ground_truth_scores)[0, 1]\n",
    "# print(f'Correlation with ground truth scores: {correlation}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
